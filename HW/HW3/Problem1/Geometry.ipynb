{"cells":[{"cell_type":"markdown","metadata":{"id":"fqzxBsqHbdLn"},"source":["# NAVARCH 565 FA 24 Homework 3"]},{"cell_type":"markdown","metadata":{"id":"HLXNDaNSbla4"},"source":["### File Submission Instructions\n","\n","#### Submit 3 Files:\n","- `Problem1.py`\n","- `Problem2.py`\n","- `YourNet.pth`\n","\n","Please submit the Python files (`Problem1.py`, `Problem2.py`) along with one trained network file (`YourNet.pth`) that you will generate from Problem 2 (see below). Make sure to name the checkpoint files as **`YourNet.pth`**, as the autograder will look for this specific filename.\n","\n","Enjoy learning Geometry and Basic Deep Learning!"]},{"cell_type":"markdown","metadata":{"id":"5AAix8ct9ac2"},"source":["# Use Google Colab or your local machine with GPU\n","Google Colab provides free cloud GPU resource with limited capacity for you to finish the perception homeworks. You can also use your local machine if GPU is available locally.\n","\n","To use Google Colab, you need to store the homework folder including this notebook in Google Drive. Go to Google Drive and double-click this `ipynb` file, and it will be opened in Google Colab automatically."]},{"cell_type":"markdown","metadata":{"id":"ZlnfHFWIbwa4"},"source":["# Setup Code\n","Before getting started we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook.\n","\n","First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) extension. This allows us to edit `.py` source files, and re-import them into the notebook for a seamless editing and debugging experience, without needing to restart the runtime."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EYwJUYLIbiTS"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"7CrsmCoSb2kl"},"source":["### Google Colab Setup\n","\n","Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n","\n","Run the following cell to mount your Google Drive. If prompted \"Permit this notebook to access your Google Drive files?\", select \"Connect to Google Drive\", and sign in to your Google account (the same account you used to store this notebook)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Eqtqkomb3OV"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"ZX9iLqkncBDe"},"source":["### Path configuration\n","Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n","\n","```\n","['Data', 'utils.py', 'Geometry.ipynb',  'Problem1.py']\n","```\n","\n","If you are working on a local machine, just set `GOOGLE_DRIVE_PATH` to the path of your `Problem1` folder."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXIon-WzcBp5"},"outputs":[],"source":["import os\n","\n","# TODO: Fill in the Google Drive path where you uploaded the assignment\n","# Example: If you create a 2024FA folder and put all the files under A3 folder, then '2024FA/A3/Problem1'\n","# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"2024FA/A3/Problem1\"\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"student 2/Problem1\"\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25x5h91pdN2F"},"outputs":[],"source":["import sys\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","\n","import time, os\n","os.environ[\"TZ\"] = \"US/Eastern\"\n","time.tzset()"]},{"cell_type":"markdown","metadata":{"id":"mZkqBEM4dOh8"},"source":["Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cells to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n","\n","```\n","Welcome to assignment 3!\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"US6sx-b4gn1c"},"outputs":[],"source":["# Imports\n","import numpy as np\n","import torch\n","import yaml\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","!pip install -q open3d\n","import open3d as o3d\n","\n","# !pip install --upgrade plotly 1>/dev/null\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xjEWdZ8XSsp"},"outputs":[],"source":["from Problem1 import *\n","from utils import *\n","hello()\n","\n","py_path = os.path.join(GOOGLE_DRIVE_PATH, 'Problem1.py')\n","py_edit_time = time.ctime(os.path.getmtime(py_path))\n","print('Problem1.py last edited on %s' % py_edit_time)"]},{"cell_type":"markdown","metadata":{"id":"dF927l-MvEQB"},"source":["### Data Visualization\n","\n","First we will load some files from the data. Check out the data folder to get familiar with some of the files. The yaml file contains information about which semantic category each integer the label corresponds to."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXyGuhQ1d5eu"},"outputs":[],"source":["# Load Semantic KITTI\n","DATA_PATH = os.path.join(GOOGLE_DRIVE_PATH, \"Data\")\n","velodyne_dir = os.path.join(DATA_PATH, 'velodyne')\n","label_dir = os.path.join(DATA_PATH, 'labels')\n","\n","frames_list = [os.path.splitext(filename)[0] for filename in sorted(os.listdir(velodyne_dir))]\n","velodyne_list = ([os.path.join(velodyne_dir, str(frame).zfill(6)+'.bin') for frame in frames_list])\n","label_list = ([os.path.join(label_dir, str(frame).zfill(6)+'.label') for frame in frames_list])\n","\n","# Label map\n","config_file = os.path.join(DATA_PATH, \"semantic_kitti.yaml\")\n","kitti_config = yaml.safe_load(open(config_file, 'r'))\n","LABELS_REMAP = kitti_config[\"learning_map\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nlubvTP4-2ob"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ZfN_P59jvTO0"},"source":["First, we will visualize a single frame of the data. This will take a few seconds to load. Once it does, move your mouse to take a look at the scene. See the gap in the middle? That is where the ego-vehicle was located. Next, let's see if we can use ICP to aggregate frames and fill the map."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bKDdbC5ivWOY"},"outputs":[],"source":["xyz_source, label_source = get_cloud(velodyne_list, label_list, 0, LABELS_REMAP)\n","plot_cloud(xyz_source, label_source)"]},{"cell_type":"markdown","metadata":{"id":"LZlHRZSUvwaI"},"source":["# Registration\n","Open the **Problem1.py** python file, and fill in the function `combine_clouds()` and `register_clouds()`. `combine_clouds()` allows us to combine sequential frames using a registration matrix. `register_clouds()` is where we will perform ICP on the raw point clouds given noisy initial guesses from odometry."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PrasgbyCwNYZ"},"outputs":[],"source":["# Register point clouds\n","start = 0\n","# Load first point cloud in sequence\n","xyz_source, label_source = get_cloud(velodyne_list, label_list, start, LABELS_REMAP)\n","xyz_prev = xyz_source\n","label_prev = label_source\n","odometry = np.loadtxt(os.path.join(DATA_PATH, 'odometry.txt'))\n","icp_transforms = []\n","# Loop through next 10 point clouds\n","for i in range(start+1, start+10):\n","  xyz_target, label_target = get_cloud(velodyne_list, label_list, i, LABELS_REMAP)\n","  # Acquire initial matrix\n","  init_mat = get_init_mat(odometry, i)\n","  # Estimate regisration matrix\n","  reg_mat = register_clouds(xyz_prev, xyz_target, trans_init=init_mat)\n","  icp_transforms.append(reg_mat)\n","  # Combine the point clouds\n","  xyz_source, label_source = combine_clouds(xyz_source, xyz_target, label_source, label_target, reg_mat)\n","  xyz_prev = xyz_target\n","# Plot registered clouds\n","plot_cloud(xyz_source, label_source)"]},{"cell_type":"markdown","metadata":{"id":"TzGUzjY2z-gf"},"source":["Looks great! However, now there are traces left behind by moving vehicles. This could be throwing off our ICP, so implement the function `mask_static()` to remove dynamic object points from the point cloud using their labels. Also implement `mask_dynamic()` to only return the points corresponding to dynamic classes. Check the yaml file to identify the integer corresponding to the car and bus classes, and remove from the input point clouds."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QHTgMUpXvMXo"},"outputs":[],"source":["# Register point clouds\n","start = 0\n","xyz_source, label_source = get_cloud(velodyne_list, label_list, start, LABELS_REMAP)\n","xyz_prev = xyz_source\n","label_prev = label_source\n","odometry = np.loadtxt(os.path.join(DATA_PATH, 'odometry.txt'))\n","icp_transforms = []\n","for i in range(start+1, start+10):\n","  xyz_target, label_target = get_cloud(velodyne_list, label_list, i, LABELS_REMAP)\n","  # remove dynamic objects\n","  static_prev, __ = mask_static(xyz_prev, label_prev)\n","  static_target, __ = mask_static(xyz_target, label_target)\n","  init_mat = get_init_mat(odometry, i)\n","  # Register without dynamic objects\n","  reg_mat = register_clouds(static_prev, static_target, trans_init=init_mat)\n","  icp_transforms.append(reg_mat)\n","  xyz_source, label_source = combine_clouds(xyz_source, xyz_target, label_source, label_target, reg_mat)\n","  xyz_prev = xyz_target\n","  label_prev = label_target\n","plot_cloud(xyz_source, label_source)"]},{"cell_type":"markdown","metadata":{"id":"hpZOIgVrrSGI"},"source":["# Instance Segmentation\n","Looks better, however there are still traces. Let's take a closer look at the moving vehicles by creating a mask."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"th31Wgc-GoUM"},"outputs":[],"source":["# Get Instances\n","def get_instances(xyz, label):\n","  road_mask = (xyz[:, 1] <= 13) & (xyz[:, 1] >= -4)\n","  xyz = xyz[road_mask, :]\n","  label = label[road_mask]\n","  xyz_dynamic, label_dynamic = mask_dynamic(xyz, label)\n","  return xyz_dynamic, label_dynamic\n","\n","# Get 10th point cloud\n","xyz_10, label_10 = get_cloud(velodyne_list, label_list, 10, LABELS_REMAP)\n","# Only plot cars on road\n","xyz_dynamic, label_dynamic = get_instances(xyz_10, label_10)\n","plot_cloud(xyz_dynamic, label_dynamic)"]},{"cell_type":"markdown","metadata":{"id":"NqFMcLOMrjgB"},"source":["There seem to be several distinct clusters of points belonging to instances of vehicles on the road. Implement functions `cluster_dists()`, `new_centroids()`, and `num_instances()` for the kMeans algorithm in the python file to identify instances in an unsupervised manner. Pay attention to how many distinct instances you see, this will be important for initialization of the algorithm."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AqmGNM2M6AKr"},"outputs":[],"source":["__, clustered_labels = cluster(xyz_dynamic)\n","# Plot clusters\n","plot_cloud(xyz_dynamic, clustered_labels+1)"]},{"cell_type":"markdown","metadata":{"id":"Ty9z_9yE0VaQ"},"source":["Next we will visualize the same scene, however without traces."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"V9w1dXhmJJzG"},"outputs":[],"source":["# Create scene without traces\n","start = 0\n","xyz_source, label_source = get_cloud(velodyne_list, label_list, start, LABELS_REMAP)\n","xyz_prev = xyz_source\n","label_prev = label_source\n","odometry = np.loadtxt(os.path.join(DATA_PATH, 'odometry.txt'))\n","for i in range(start+1, start+10):\n","  xyz_target, label_target = get_cloud(velodyne_list, label_list, i, LABELS_REMAP)\n","  static_prev, __ = mask_static(xyz_prev, label_prev)\n","  static_target, __ = mask_static(xyz_target, label_target)\n","  reg_mat = icp_transforms[i-1]\n","  xyz_source, label_source = combine_clouds(xyz_source, xyz_target, label_source, label_target, reg_mat)\n","  xyz_prev = xyz_target\n","  label_prev = label_target\n","\n","xyz_static, label_static = mask_static(xyz_source, label_source)\n","xyz_10, label_10 = get_cloud(velodyne_list, label_list, 10, LABELS_REMAP)\n","xyz_dynamic, __ = get_instances(xyz_10, label_10)\n","__, label_dynamic = cluster(xyz_dynamic)\n","xyz_static, label_static = downsample_cloud(xyz_static, label_static, 100000 - label_dynamic.shape[0])\n","xyz_all, label_all = combine_clouds(xyz_static, xyz_dynamic, label_static, label_dynamic, np.eye(4))\n","\n","plot_cloud(xyz_all, label_all)"]},{"cell_type":"markdown","metadata":{"id":"B0kZZoEM0ZiN"},"source":["Looks much better, however there are still ways to improve. Can you think of methods to construct more complete instance point clouds using the tools you just learned?"]},{"cell_type":"markdown","metadata":{"id":"n9VP2s1xaf90"},"source":["# LiDAR to Camera Transformations\n","Recall from class the distinct advantages and disadvantages of each sensor. In adverse driving conditions, it is even more important to leverage a full sensor suite. As the final step in Problem 1, we will be transforming LiDAR points to pixels on data from the off-road driving dataset RELLIS-3D.\n","\n","First, we will visualize the image without LiDAR points below."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kjAErEvsfYTT"},"outputs":[],"source":["image = cv2.imread(os.path.join(GOOGLE_DRIVE_PATH, \"Data\", \"RELLIS\", \"Camera.jpg\"))\n","res = print_projection_plt(image=image)\n","\n","plt.subplots(1,1, figsize = (20,20) )\n","plt.title(\"Camera without Velodyne\")\n","plt.imshow(res)"]},{"cell_type":"markdown","metadata":{"id":"VVyMSmYwgfMy"},"source":["Next, implement `to_pixels()` which will receive 3D LiDAR points (Nx3), the intrinsic matrix, and a transformation from LiDAR frame to camera frame. Return the pixel values as a Nx2 matrix. Also return the depth for each point by transforming to camera frame, then obtaining depth from the third column. If this works correctly, you should see points which generally match the layout of the scene. Note that LiDAR is sparseand sensitive to noise from vegetation, so the points may not match up exactly."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Kq_gNEP9dLSu"},"outputs":[],"source":["# Camera intrinsic matrix\n","P = np.array([[2.81364327e+03, 0.00000000e+00, 9.69285772e+02],\n","              [0.00000000e+00, 2.80832608e+03, 6.24049972e+02],\n","              [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])\n","\n","# Transform from LiDAR to camera\n","RT = np.array([[0.03462247,  0.99936055, -0.00893175, -0.03566209],\n","               [ 0.00479177, -0.00910301, -0.99994709, -0.17154603],\n","               [-0.99938898,  0.03457784, -0.00510388, -0.13379309],\n","               [ 0.,          0.,          0.,          1.        ]])\n","\n","fpath = os.path.join(GOOGLE_DRIVE_PATH, \"Data\", \"RELLIS\", \"pc.bin\")\n","pc = np.fromfile(fpath, dtype=np.float64).reshape(-1, 3)\n","imgpoints, d = to_pixels(pc, P, RT)\n","\n","# Convert depth to color and remove invalid points\n","imgpoints = imgpoints.T\n","mask = (imgpoints[0, :] > 0) & (imgpoints[1, :] > 0)\n","imgpoints = imgpoints[:, mask]\n","d = d[mask]\n","c_ = depth_color(d)\n","\n","# Plot image\n","res = print_projection_plt(image, points=imgpoints, color=c_)\n","plt.subplots(1,1, figsize = (20,20) )\n","plt.title(\"Velodyne points to camera image Result\")\n","plt.imshow(res)"]},{"cell_type":"markdown","metadata":{"id":"HnBZTylEaryn"},"source":["# Time to learn!\n","In the next part of the homework, we will begin our learning journey starting with image classification for autonomous vehicles."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-Npbm7l3RS1T"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}